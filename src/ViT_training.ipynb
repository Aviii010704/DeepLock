{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5egdCCepwtxR",
        "outputId": "551c7a2a-03af-4206-a79d-bb876f669f64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRyxOU9MxFnY",
        "outputId": "72d3beac-0980-4d85-d74f-e44a55191ab5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: colorama\n",
            "Successfully installed colorama-0.4.6\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-e2YeL0xwy-j"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler, TensorDataset\n",
        "from tqdm import tqdm\n",
        "from colorama import Fore, Style\n",
        "import os\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qlepb0QtwwHH"
      },
      "outputs": [],
      "source": [
        "#model.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"Split image into patches and embed them.\"\"\"\n",
        "    def __init__(self, image_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "\n",
        "        self.projection = nn.Sequential(\n",
        "            # Convert image into patches and flatten\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)',\n",
        "                      p1=patch_size, p2=patch_size),\n",
        "            nn.Linear(patch_size * patch_size * in_channels, embed_dim)\n",
        "        )\n",
        "\n",
        "        # Add learnable classification token\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "\n",
        "        # Add learnable position embeddings\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, embed_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b = x.shape[0]  # batch size\n",
        "        x = self.projection(x)\n",
        "\n",
        "        # Add classification token to each sequence\n",
        "        cls_tokens = self.cls_token.expand(b, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        # Add position embeddings\n",
        "        x = x + self.pos_embedding\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-head self-attention mechanism.\"\"\"\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
        "        self.att_drop = nn.Dropout(0.1)\n",
        "        self.projection = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, num_patches, embed_dim = x.shape\n",
        "\n",
        "        qkv = self.qkv(x)\n",
        "        qkv = qkv.reshape(batch_size, num_patches, 3, self.num_heads, self.head_dim)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        # Attention\n",
        "        att = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.att_drop(att)\n",
        "\n",
        "        x = (att @ v).transpose(1, 2).reshape(batch_size, num_patches, embed_dim)\n",
        "        x = self.projection(x)\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Transformer block with attention and MLP.\"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = MultiHeadAttention(embed_dim, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(mlp_hidden_dim, embed_dim),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    \"\"\"Vision Transformer for binary classification.\"\"\"\n",
        "    def __init__(self,\n",
        "                 image_size=224,\n",
        "                 patch_size=16,\n",
        "                 in_channels=3,\n",
        "                 embed_dim=768,\n",
        "                 num_layers=12,\n",
        "                 num_heads=12,\n",
        "                 mlp_ratio=4.0,\n",
        "                 dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Patch Embedding\n",
        "        self.patch_embed = PatchEmbedding(\n",
        "            image_size=image_size,\n",
        "            patch_size=patch_size,\n",
        "            in_channels=in_channels,\n",
        "            embed_dim=embed_dim\n",
        "        )\n",
        "\n",
        "        # Transformer Encoder\n",
        "        self.transformer = nn.Sequential(*[\n",
        "            TransformerBlock(embed_dim, num_heads, mlp_ratio)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Classification Head\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.LayerNorm(embed_dim),\n",
        "            nn.Linear(embed_dim, 1)  # Binary classification\n",
        "        )\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(m.weight)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Patch embedding\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        # Apply dropout\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Transformer blocks\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        # Classification token\n",
        "        x = self.norm(x)\n",
        "        x = x[:, 0]  # Use [CLS] token\n",
        "\n",
        "        # Classification head\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"Split image into patches and embed them.\"\"\"\n",
        "    def __init__(self, image_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "\n",
        "        self.projection = nn.Sequential(\n",
        "            # Convert image into patches and flatten\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)',\n",
        "                      p1=patch_size, p2=patch_size),\n",
        "            nn.Linear(patch_size * patch_size * in_channels, embed_dim)\n",
        "        )\n",
        "\n",
        "        # Add learnable classification token\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "\n",
        "        # Add learnable position embeddings\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, embed_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b = x.shape[0]  # batch size\n",
        "        x = self.projection(x)\n",
        "\n",
        "        # Add classification token to each sequence\n",
        "        cls_tokens = self.cls_token.expand(b, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        # Add position embeddings\n",
        "        x = x + self.pos_embedding\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-head self-attention mechanism.\"\"\"\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
        "        self.att_drop = nn.Dropout(0.1)\n",
        "        self.projection = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, num_patches, embed_dim = x.shape\n",
        "\n",
        "        qkv = self.qkv(x)\n",
        "        qkv = qkv.reshape(batch_size, num_patches, 3, self.num_heads, self.head_dim)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        # Attention\n",
        "        att = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.att_drop(att)\n",
        "\n",
        "        x = (att @ v).transpose(1, 2).reshape(batch_size, num_patches, embed_dim)\n",
        "        x = self.projection(x)\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Transformer block with attention and MLP.\"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = MultiHeadAttention(embed_dim, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(mlp_hidden_dim, embed_dim),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"Split image into patches and embed them.\"\"\"\n",
        "    def __init__(self, image_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "\n",
        "        self.projection = nn.Sequential(\n",
        "            # Convert image into patches and flatten\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)',\n",
        "                      p1=patch_size, p2=patch_size),\n",
        "            nn.Linear(patch_size * patch_size * in_channels, embed_dim)\n",
        "        )\n",
        "\n",
        "        # Add learnable classification token\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "\n",
        "        # Add learnable position embeddings\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, embed_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b = x.shape[0]  # batch size\n",
        "        x = self.projection(x)\n",
        "\n",
        "        # Add classification token to each sequence\n",
        "        cls_tokens = self.cls_token.expand(b, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        # Add position embeddings\n",
        "        x = x + self.pos_embedding\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-head self-attention mechanism.\"\"\"\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
        "        self.att_drop = nn.Dropout(0.1)\n",
        "        self.projection = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, num_patches, embed_dim = x.shape\n",
        "\n",
        "        qkv = self.qkv(x)\n",
        "        qkv = qkv.reshape(batch_size, num_patches, 3, self.num_heads, self.head_dim)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        # Attention\n",
        "        att = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.att_drop(att)\n",
        "\n",
        "        x = (att @ v).transpose(1, 2).reshape(batch_size, num_patches, embed_dim)\n",
        "        x = self.projection(x)\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Transformer block with attention and MLP.\"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = MultiHeadAttention(embed_dim, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(mlp_hidden_dim, embed_dim),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"Split image into patches and embed them.\"\"\"\n",
        "    def __init__(self, image_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "\n",
        "        self.projection = nn.Sequential(\n",
        "            # Convert image into patches and flatten\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)',\n",
        "                      p1=patch_size, p2=patch_size),\n",
        "            nn.Linear(patch_size * patch_size * in_channels, embed_dim)\n",
        "        )\n",
        "\n",
        "        # Add learnable classification token\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "\n",
        "        # Add learnable position embeddings\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, embed_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b = x.shape[0]  # batch size\n",
        "        x = self.projection(x)\n",
        "\n",
        "        # Add classification token to each sequence\n",
        "        cls_tokens = self.cls_token.expand(b, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        # Add position embeddings\n",
        "        x = x + self.pos_embedding\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-head self-attention mechanism.\"\"\"\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
        "        self.att_drop = nn.Dropout(0.1)\n",
        "        self.projection = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, num_patches, embed_dim = x.shape\n",
        "\n",
        "        qkv = self.qkv(x)\n",
        "        qkv = qkv.reshape(batch_size, num_patches, 3, self.num_heads, self.head_dim)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        # Attention\n",
        "        att = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.att_drop(att)\n",
        "\n",
        "        x = (att @ v).transpose(1, 2).reshape(batch_size, num_patches, embed_dim)\n",
        "        x = self.projection(x)\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Transformer block with attention and MLP.\"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = MultiHeadAttention(embed_dim, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(mlp_hidden_dim, embed_dim),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    \"\"\"Vision Transformer for binary classification.\"\"\"\n",
        "    def __init__(self,\n",
        "                 image_size=224,\n",
        "                 patch_size=16,\n",
        "                 in_channels=3,\n",
        "                 embed_dim=768,\n",
        "                 num_layers=12,\n",
        "                 num_heads=12,\n",
        "                 mlp_ratio=4.0,\n",
        "                 dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Patch Embedding\n",
        "        self.patch_embed = PatchEmbedding(\n",
        "            image_size=image_size,\n",
        "            patch_size=patch_size,\n",
        "            in_channels=in_channels,\n",
        "            embed_dim=embed_dim\n",
        "        )\n",
        "\n",
        "        # Transformer Encoder\n",
        "        self.transformer = nn.Sequential(*[\n",
        "            TransformerBlock(embed_dim, num_heads, mlp_ratio)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Classification Head\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.LayerNorm(embed_dim),\n",
        "            nn.Linear(embed_dim, 1)  # Binary classification\n",
        "        )\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(m.weight)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Patch embedding\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        # Apply dropout\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Transformer blocks\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        # Classification token\n",
        "        x = self.norm(x)\n",
        "        x = x[:, 0]  # Use [CLS] token\n",
        "\n",
        "        # Classification head\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "def get_vit(pretrained=False):\n",
        "    \"\"\"Returns an initialized ViT model for binary classification.\"\"\"\n",
        "    model = ViT(\n",
        "        image_size=224,        # Required input size\n",
        "        patch_size=16,         # Size of patches\n",
        "        in_channels=3,         # RGB images\n",
        "        embed_dim=768,         # Embedding dimension\n",
        "        num_layers=12,         # Number of transformer blocks\n",
        "        num_heads=12,          # Number of attention heads\n",
        "        mlp_ratio=4.0,         # MLP hidden dimension ratio\n",
        "        dropout=0.1            # Dropout rate\n",
        "    )\n",
        "\n",
        "    if pretrained:\n",
        "        # You would typically load pretrained weights here\n",
        "        # This is left as a placeholder\n",
        "        pass\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dxI5WZAryQTS"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ‚úÖ Paths\n",
        "DATA_PATH = r\"C:\\Users\\Aviral\\Desktop\\DeepLock\\data\\preprocessed_data_p1.pth\"  # your .pth dataset\n",
        "MODEL_PATH = r\"C:\\Users\\Aviral\\Desktop\\DeepLock\\models\\Final Models\\deeplock_vit.pth\"\n",
        "LOG_PATH = r\"C:\\Users\\Aviral\\Desktop\\DeepLock\\logs_vit.json\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0WkpNZNycoi",
        "outputId": "3a9791f7-6424-4f6b-f330-d23ddcaf8f5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÇ Loading dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Aviral\\AppData\\Local\\Temp\\ipykernel_29080\\962665023.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  X, y = torch.load(DATA_PATH)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m‚úÖ Dataset loaded: 10190 samples\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ‚úÖ Load Dataset\n",
        "print(\"üìÇ Loading dataset...\")\n",
        "X, y = torch.load(DATA_PATH)\n",
        "dataset = TensorDataset(X, y)\n",
        "print(f\"{Fore.CYAN}‚úÖ Dataset loaded: {len(dataset)} samples{Style.RESET_ALL}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5nheryWwkgu",
        "outputId": "727ab868-f4d7-4098-b74f-c504b161825e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m‚Üí Initializing new ViT model (no checkpoint found)\u001b[0m\n",
            "\n",
            "Epoch 1/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32mTrain\u001b[0m:   0%|\u001b[32m                                                                \u001b[0m| 0/255 [00:00<?, ?it/s]\u001b[0m\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Couldn't open shared file mapping: <torch_29080_4099347938_0>, error code: <1450>",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 72\u001b[0m\n\u001b[0;32m     63\u001b[0m train_loss, correct_train, total_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     65\u001b[0m train_progress \u001b[38;5;241m=\u001b[39m tqdm(\n\u001b[0;32m     66\u001b[0m     train_loader,\n\u001b[0;32m     67\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFore\u001b[38;5;241m.\u001b[39mGREEN\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mStyle\u001b[38;5;241m.\u001b[39mRESET_ALL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     68\u001b[0m     ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m     69\u001b[0m     bar_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{l_bar}\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;132;01m{bar}\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;132;01m{r_bar}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (Fore\u001b[38;5;241m.\u001b[39mGREEN, Style\u001b[38;5;241m.\u001b[39mRESET_ALL)\n\u001b[0;32m     70\u001b[0m )\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_progress:\n\u001b[0;32m     73\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     74\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\Aviral\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
            "File \u001b[1;32mc:\\Users\\Aviral\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:484\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Aviral\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:415\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Aviral\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1138\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1131\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1138\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
            "File \u001b[1;32mc:\\Users\\Aviral\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Aviral\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Aviral\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\context.py:337\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Aviral\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\popen_spawn_win32.py:95\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 95\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\Aviral\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Aviral\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\multiprocessing\\reductions.py:607\u001b[0m, in \u001b[0;36mreduce_storage\u001b[1;34m(storage)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot pickle meta storage; try pickling a meta tensor instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    605\u001b[0m     )\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m get_sharing_strategy() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_system\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 607\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_share_filename_cpu_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    608\u001b[0m     cache_key \u001b[38;5;241m=\u001b[39m metadata[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    609\u001b[0m     rebuild \u001b[38;5;241m=\u001b[39m rebuild_storage_filename\n",
            "File \u001b[1;32mc:\\Users\\Aviral\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\storage.py:437\u001b[0m, in \u001b[0;36m_share_memory_lock_protected.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    434\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;66;03m# If we acquired the storage lock here and we're done working on it\u001b[39;00m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;66;03m# we can now release it and free the entry.\u001b[39;00m\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m to_free \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    442\u001b[0m         \u001b[38;5;66;03m# Ensure that the cdata from the storage didn't change and only\u001b[39;00m\n\u001b[0;32m    443\u001b[0m         \u001b[38;5;66;03m# the data_ptr did.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Aviral\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\storage.py:516\u001b[0m, in \u001b[0;36mUntypedStorage._share_filename_cpu_\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;129m@_share_memory_lock_protected\u001b[39m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_share_filename_cpu_\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_share_filename_cpu_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Couldn't open shared file mapping: <torch_29080_4099347938_0>, error code: <1450>"
          ]
        }
      ],
      "source": [
        "\n",
        "# from model import get_vit  # make sure this returns a ViT model with output 1\n",
        "\n",
        "# ‚úÖ Paths\n",
        "# DATA_PATH = \"/content/drive/MyDrive/DEEPLOCK/preprocessed_data_p1.pth\"  # your .pth dataset\n",
        "# MODEL_PATH = \"/content/drive/MyDrive/DEEPLOCK/deeplock_vit.pth\"\n",
        "# LOG_PATH = \"/content/drive/MyDrive/DEEPLOCK/logs_vit.json\"\n",
        "\n",
        "# ‚úÖ Configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "EPOCHS = 40\n",
        "BATCH_SIZE = 16\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "# ‚úÖ Initialize Model\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    model = get_vit(pretrained=False).to(device)\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "    print(f\"{Fore.YELLOW}‚Üí Loaded existing ViT model from '{MODEL_PATH}'{Style.RESET_ALL}\")\n",
        "else:\n",
        "    model = get_vit(pretrained=True).to(device)\n",
        "    print(f\"{Fore.YELLOW}‚Üí Initializing new ViT model (no checkpoint found){Style.RESET_ALL}\")\n",
        "\n",
        "# ‚úÖ Train/Val Split\n",
        "total_samples = len(dataset)\n",
        "train_size = int(0.8 * total_samples)\n",
        "train_indices = list(range(train_size))\n",
        "val_indices = list(range(train_size, total_samples))\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    sampler=SubsetRandomSampler(train_indices),\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=False\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    sampler=SubsetRandomSampler(val_indices),\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=False\n",
        ")\n",
        "\n",
        "# ‚úÖ Optimizer, Scheduler, Loss\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=2e-3,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=EPOCHS\n",
        ")\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# ‚úÖ Training Loop\n",
        "best_acc = 0.0\n",
        "logs = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    # --- Training ---\n",
        "    model.train()\n",
        "    train_loss, correct_train, total_train = 0.0, 0, 0\n",
        "\n",
        "    train_progress = tqdm(\n",
        "        train_loader,\n",
        "        desc=f\"{Fore.GREEN}Train{Style.RESET_ALL}\",\n",
        "        ncols=100,\n",
        "        bar_format=\"{l_bar}%s{bar}%s{r_bar}\" % (Fore.GREEN, Style.RESET_ALL)\n",
        "    )\n",
        "\n",
        "    for images, labels in train_progress:\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        labels = labels.float().unsqueeze(1).to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        preds = torch.sigmoid(outputs) > 0.5\n",
        "        correct_train += (preds == labels).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "        train_progress.set_postfix({\n",
        "            'loss': f\"{train_loss / (total_train / images.size(0)):.4f}\",\n",
        "            'acc': f\"{100 * correct_train / total_train:.2f}%\"\n",
        "        })\n",
        "\n",
        "    # --- Validation ---\n",
        "    model.eval()\n",
        "    val_loss, correct_val, total_val = 0.0, 0, 0\n",
        "\n",
        "    val_progress = tqdm(\n",
        "        val_loader,\n",
        "        desc=f\"{Fore.CYAN}Val{Style.RESET_ALL}\",\n",
        "        ncols=100,\n",
        "        bar_format=\"{l_bar}%s{bar}%s{r_bar}\" % (Fore.CYAN, Style.RESET_ALL)\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_progress:\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            labels = labels.float().unsqueeze(1).to(device, non_blocking=True)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            preds = torch.sigmoid(outputs) > 0.5\n",
        "            correct_val += (preds == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "\n",
        "            val_progress.set_postfix({\n",
        "                'loss': f\"{val_loss / (total_val / images.size(0)):.4f}\",\n",
        "                'acc': f\"{100 * correct_val / total_val:.2f}%\"\n",
        "            })\n",
        "\n",
        "    # ‚úÖ Save Best Model\n",
        "    val_acc = 100 * correct_val / total_val\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), MODEL_PATH)\n",
        "        print(f\"{Fore.GREEN}‚Üí üèÜ New best model saved! Validation accuracy: {val_acc:.2f}%{Style.RESET_ALL}\")\n",
        "\n",
        "    logs.append({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"train_loss\": train_loss / len(train_loader),\n",
        "        \"train_acc\": 100 * correct_train / total_train,\n",
        "        \"val_loss\": val_loss / len(val_loader),\n",
        "        \"val_acc\": val_acc\n",
        "    })\n",
        "\n",
        "    print()\n",
        "\n",
        "# ‚úÖ Save Logs\n",
        "with open(LOG_PATH, \"w\") as f:\n",
        "    json.dump(logs, f, indent=4)\n",
        "\n",
        "print(f\"‚úÖ Training complete! Best validation accuracy: {best_acc:.2f}%\")\n",
        "print(f\"Model saved to: {MODEL_PATH}\")\n",
        "print(f\"Logs saved to: {LOG_PATH}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
